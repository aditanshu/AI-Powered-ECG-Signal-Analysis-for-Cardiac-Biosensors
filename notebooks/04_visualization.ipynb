{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ECG Visualization and Analysis\n",
                "\n",
                "This notebook provides comprehensive visualizations for ECG signal analysis results.\n",
                "\n",
                "## Visualizations:\n",
                "1. Signal quality assessment\n",
                "2. Feature distributions\n",
                "3. Model predictions vs actual\n",
                "4. Confusion matrices\n",
                "5. ROC curves and performance metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
                "from sklearn.preprocessing import label_binarize\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set style\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette(\"husl\")\n",
                "\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generate Sample Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate synthetic results for demonstration\n",
                "np.random.seed(42)\n",
                "\n",
                "n_samples = 1000\n",
                "n_classes = 5\n",
                "class_names = ['Normal', 'PVC', 'PAC', 'VF', 'VT']\n",
                "\n",
                "# True labels\n",
                "y_true = np.random.randint(0, n_classes, n_samples)\n",
                "\n",
                "# Predicted labels (with some errors)\n",
                "y_pred = y_true.copy()\n",
                "error_indices = np.random.choice(n_samples, size=int(n_samples * 0.15), replace=False)\n",
                "y_pred[error_indices] = np.random.randint(0, n_classes, len(error_indices))\n",
                "\n",
                "# Prediction probabilities\n",
                "y_proba = np.random.rand(n_samples, n_classes)\n",
                "y_proba = y_proba / y_proba.sum(axis=1, keepdims=True)\n",
                "\n",
                "print(f\"Generated {n_samples} samples with {n_classes} classes\")\n",
                "print(f\"Accuracy: {np.mean(y_true == y_pred):.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute confusion matrix\n",
                "cm = confusion_matrix(y_true, y_pred)\n",
                "\n",
                "# Plot confusion matrix\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "           xticklabels=class_names, yticklabels=class_names,\n",
                "           cbar_kws={'label': 'Count'})\n",
                "plt.title('Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
                "plt.ylabel('True Label', fontsize=12)\n",
                "plt.xlabel('Predicted Label', fontsize=12)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Normalized confusion matrix\n",
                "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='RdYlGn', \n",
                "           xticklabels=class_names, yticklabels=class_names,\n",
                "           cbar_kws={'label': 'Percentage'})\n",
                "plt.title('Normalized Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
                "plt.ylabel('True Label', fontsize=12)\n",
                "plt.xlabel('Predicted Label', fontsize=12)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Classification Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate classification report\n",
                "report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
                "report_df = pd.DataFrame(report).transpose()\n",
                "\n",
                "print(\"Classification Report:\")\n",
                "print(report_df)\n",
                "\n",
                "# Visualize metrics\n",
                "metrics_df = report_df.iloc[:-3, :3]  # Exclude avg rows and support column\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "\n",
                "# Precision\n",
                "axes[0].bar(range(len(metrics_df)), metrics_df['precision'], \n",
                "           color='#E63946', alpha=0.7, edgecolor='black')\n",
                "axes[0].set_xticks(range(len(metrics_df)))\n",
                "axes[0].set_xticklabels(metrics_df.index, rotation=45)\n",
                "axes[0].set_title('Precision by Class', fontsize=14, fontweight='bold')\n",
                "axes[0].set_ylabel('Precision', fontsize=12)\n",
                "axes[0].set_ylim([0, 1])\n",
                "axes[0].grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "# Recall\n",
                "axes[1].bar(range(len(metrics_df)), metrics_df['recall'], \n",
                "           color='#4361EE', alpha=0.7, edgecolor='black')\n",
                "axes[1].set_xticks(range(len(metrics_df)))\n",
                "axes[1].set_xticklabels(metrics_df.index, rotation=45)\n",
                "axes[1].set_title('Recall by Class', fontsize=14, fontweight='bold')\n",
                "axes[1].set_ylabel('Recall', fontsize=12)\n",
                "axes[1].set_ylim([0, 1])\n",
                "axes[1].grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "# F1-Score\n",
                "axes[2].bar(range(len(metrics_df)), metrics_df['f1-score'], \n",
                "           color='#06A77D', alpha=0.7, edgecolor='black')\n",
                "axes[2].set_xticks(range(len(metrics_df)))\n",
                "axes[2].set_xticklabels(metrics_df.index, rotation=45)\n",
                "axes[2].set_title('F1-Score by Class', fontsize=14, fontweight='bold')\n",
                "axes[2].set_ylabel('F1-Score', fontsize=12)\n",
                "axes[2].set_ylim([0, 1])\n",
                "axes[2].grid(True, alpha=0.3, axis='y')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ROC Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Binarize labels for ROC curve\n",
                "y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
                "\n",
                "# Compute ROC curve and AUC for each class\n",
                "fpr = dict()\n",
                "tpr = dict()\n",
                "roc_auc = dict()\n",
                "\n",
                "for i in range(n_classes):\n",
                "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_proba[:, i])\n",
                "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
                "\n",
                "# Plot ROC curves\n",
                "plt.figure(figsize=(10, 8))\n",
                "colors = ['#E63946', '#F77F00', '#4361EE', '#06A77D', '#9D4EDD']\n",
                "\n",
                "for i, color in zip(range(n_classes), colors):\n",
                "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
                "            label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
                "\n",
                "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')\n",
                "plt.xlim([0.0, 1.0])\n",
                "plt.ylim([0.0, 1.05])\n",
                "plt.xlabel('False Positive Rate', fontsize=12)\n",
                "plt.ylabel('True Positive Rate', fontsize=12)\n",
                "plt.title('ROC Curves for Multi-Class Classification', fontsize=14, fontweight='bold')\n",
                "plt.legend(loc=\"lower right\")\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Print AUC scores\n",
                "print(\"\\nAUC Scores:\")\n",
                "for i in range(n_classes):\n",
                "    print(f\"  {class_names[i]}: {roc_auc[i]:.4f}\")\n",
                "print(f\"  Average: {np.mean(list(roc_auc.values())):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Prediction Confidence Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze prediction confidence\n",
                "max_proba = np.max(y_proba, axis=1)\n",
                "correct_mask = (y_true == y_pred)\n",
                "\n",
                "# Plot confidence distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "# Confidence for correct vs incorrect predictions\n",
                "axes[0].hist(max_proba[correct_mask], bins=50, alpha=0.7, \n",
                "            color='#06A77D', label='Correct', edgecolor='black')\n",
                "axes[0].hist(max_proba[~correct_mask], bins=50, alpha=0.7, \n",
                "            color='#E63946', label='Incorrect', edgecolor='black')\n",
                "axes[0].set_xlabel('Prediction Confidence', fontsize=12)\n",
                "axes[0].set_ylabel('Frequency', fontsize=12)\n",
                "axes[0].set_title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Box plot by class\n",
                "confidence_by_class = [max_proba[y_true == i] for i in range(n_classes)]\n",
                "bp = axes[1].boxplot(confidence_by_class, labels=class_names, patch_artist=True)\n",
                "for patch, color in zip(bp['boxes'], colors):\n",
                "    patch.set_facecolor(color)\n",
                "    patch.set_alpha(0.7)\n",
                "axes[1].set_xlabel('Class', fontsize=12)\n",
                "axes[1].set_ylabel('Prediction Confidence', fontsize=12)\n",
                "axes[1].set_title('Confidence by True Class', fontsize=14, fontweight='bold')\n",
                "axes[1].grid(True, alpha=0.3, axis='y')\n",
                "plt.xticks(rotation=45)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"Average confidence (correct): {np.mean(max_proba[correct_mask]):.4f}\")\n",
                "print(f\"Average confidence (incorrect): {np.mean(max_proba[~correct_mask]):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Class Distribution Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze class distribution\n",
                "true_counts = np.bincount(y_true)\n",
                "pred_counts = np.bincount(y_pred)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
                "\n",
                "# True distribution\n",
                "axes[0].bar(class_names, true_counts, color=colors, alpha=0.7, edgecolor='black')\n",
                "axes[0].set_title('True Class Distribution', fontsize=14, fontweight='bold')\n",
                "axes[0].set_ylabel('Count', fontsize=12)\n",
                "axes[0].grid(True, alpha=0.3, axis='y')\n",
                "plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=45)\n",
                "\n",
                "# Predicted distribution\n",
                "axes[1].bar(class_names, pred_counts, color=colors, alpha=0.7, edgecolor='black')\n",
                "axes[1].set_title('Predicted Class Distribution', fontsize=14, fontweight='bold')\n",
                "axes[1].set_ylabel('Count', fontsize=12)\n",
                "axes[1].grid(True, alpha=0.3, axis='y')\n",
                "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Error Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze misclassifications\n",
                "misclassified = y_true != y_pred\n",
                "misclass_df = pd.DataFrame({\n",
                "    'True': [class_names[i] for i in y_true[misclassified]],\n",
                "    'Predicted': [class_names[i] for i in y_pred[misclassified]],\n",
                "    'Confidence': max_proba[misclassified]\n",
                "})\n",
                "\n",
                "print(f\"Total misclassifications: {len(misclass_df)}\")\n",
                "print(\"\\nMost common misclassifications:\")\n",
                "print(misclass_df.groupby(['True', 'Predicted']).size().sort_values(ascending=False).head(10))\n",
                "\n",
                "# Visualize error patterns\n",
                "error_matrix = np.zeros((n_classes, n_classes))\n",
                "for true, pred in zip(y_true[misclassified], y_pred[misclassified]):\n",
                "    error_matrix[true, pred] += 1\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(error_matrix, annot=True, fmt='.0f', cmap='Reds',\n",
                "           xticklabels=class_names, yticklabels=class_names,\n",
                "           cbar_kws={'label': 'Error Count'})\n",
                "plt.title('Error Pattern Matrix', fontsize=16, fontweight='bold', pad=20)\n",
                "plt.ylabel('True Label', fontsize=12)\n",
                "plt.xlabel('Predicted Label', fontsize=12)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "This notebook demonstrated:\n",
                "- Confusion matrix visualization (raw and normalized)\n",
                "- Classification metrics (precision, recall, F1-score)\n",
                "- ROC curves and AUC scores for multi-class classification\n",
                "- Prediction confidence analysis\n",
                "- Class distribution comparison\n",
                "- Error pattern analysis\n",
                "\n",
                "These visualizations provide comprehensive insights into model performance and areas for improvement!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}